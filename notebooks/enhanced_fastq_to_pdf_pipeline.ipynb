{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced FASTQ-to-PDF Pipeline: Single Horse Analysis\n",
    "\n",
    "**Enhanced pipeline for equine microbiome analysis with statistical validation**\n",
    "\n",
    "This notebook implements the enhanced FASTQ-to-PDF pipeline that properly handles technical replicates as a single horse analysis, not separate reports. Based on Gemini feedback, this version includes:\n",
    "\n",
    "‚úÖ **Statistical validation** of technical replicates (Spearman correlation)  \n",
    "‚úÖ **Proper normalization** (relative abundance, CPM, or rarefaction)  \n",
    "‚úÖ **Quality control** metrics and warnings  \n",
    "‚úÖ **Single comprehensive report** per patient  \n",
    "‚úÖ **Enhanced error handling** and logging  \n",
    "\n",
    "## Key Improvements Over Original Pipeline\n",
    "\n",
    "- **Single Horse Paradigm**: Multiple barcodes = technical replicates of same horse\n",
    "- **Statistical Rigor**: Correlation validation (r ‚â• 0.7, p ‚â§ 0.05)\n",
    "- **Data Normalization**: Proper abundance normalization before aggregation\n",
    "- **Quality Metrics**: Comprehensive QC reporting and validation\n",
    "- **Professional Output**: Single PDF report with combined analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced FASTQ-to-PDF Pipeline - Setup Complete\n",
      "üìÅ Project root: /home/trentleslie/Insync/projects/equine-microbiome-reporter\n",
      "üìÅ Source path: /home/trentleslie/Insync/projects/equine-microbiome-reporter/src\n",
      "‚è∞ Session started: 2025-08-04 01:28:23\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / 'src'\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import enhanced pipeline components\n",
    "from enhanced_notebook_interface import (\n",
    "    run_enhanced_pipeline, \n",
    "    PatientInfo, \n",
    "    ProcessingMode,\n",
    "    generate_simple_pdf_report\n",
    ")\n",
    "from barcode_aggregator import AggregationConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced FASTQ-to-PDF Pipeline - Setup Complete\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìÅ Source path: {src_path}\")\n",
    "print(f\"‚è∞ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Configuration\n",
    "\n",
    "Configure patient information for the single horse being analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Patient Configuration:\n",
      "   Name: Montana\n",
      "   Species: Horse\n",
      "   Age: 20 years\n",
      "   Sample Number: 004-006_combined\n",
      "   Barcode Range: barcode04-barcode06\n",
      "   Performed by: Julia Ko≈Ñczak\n",
      "   Requested by: Dr. Alexandra Matusiak\n",
      "   Notes: Combined analysis from 3 technical replicates with statistical validation\n"
     ]
    }
   ],
   "source": [
    "# Configure patient information\n",
    "patient = PatientInfo(\n",
    "    name=\"Montana\",\n",
    "    species=\"Horse\",\n",
    "    age=\"20 years\",\n",
    "    sample_number=\"004-006_combined\",\n",
    "    barcode_range=\"barcode04-barcode06\",\n",
    "    date_received=\"2024-08-04\",\n",
    "    date_analyzed=\"2024-08-04\",\n",
    "    performed_by=\"Julia Ko≈Ñczak\",\n",
    "    requested_by=\"Dr. Alexandra Matusiak\",\n",
    "    notes=\"Combined analysis from 3 technical replicates with statistical validation\"\n",
    ")\n",
    "\n",
    "print(\"üë§ Patient Configuration:\")\n",
    "print(f\"   Name: {patient.name}\")\n",
    "print(f\"   Species: {patient.species}\")\n",
    "print(f\"   Age: {patient.age}\")\n",
    "print(f\"   Sample Number: {patient.sample_number}\")\n",
    "print(f\"   Barcode Range: {patient.barcode_range}\")\n",
    "print(f\"   Performed by: {patient.performed_by}\")\n",
    "print(f\"   Requested by: {patient.requested_by}\")\n",
    "print(f\"   Notes: {patient.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Configuration\n",
    "\n",
    "Configure the enhanced processing parameters for single-horse analysis with statistical validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Processing Configuration:\n",
      "   Mode: single_horse\n",
      "   Combine barcodes: True\n",
      "   Normalization: relative_abundance\n",
      "   Correlation threshold: 0.7\n",
      "   P-value threshold: 0.05\n",
      "   Data directory: ../data\n",
      "   Barcode directories: ['barcode04', 'barcode05', 'barcode06']\n",
      "   Output directory: enhanced_results\n"
     ]
    }
   ],
   "source": [
    "# Configure processing mode for single horse analysis\n",
    "processing_mode = ProcessingMode(\n",
    "    mode=\"single_horse\",                    # Single horse analysis (not multiple)\n",
    "    combine_barcodes=True,                  # Combine technical replicates\n",
    "    normalization_method=\"relative_abundance\",  # Normalization method\n",
    "    correlation_threshold=0.7,              # Minimum correlation for valid replicates\n",
    "    p_value_threshold=0.05                  # Statistical significance threshold\n",
    ")\n",
    "\n",
    "# Data directories and barcode configuration\n",
    "data_dir = \"../data\"                       # Directory containing FASTQ files\n",
    "barcode_dirs = [\"barcode04\", \"barcode05\", \"barcode06\"]  # Technical replicates\n",
    "output_dir = \"enhanced_results\"            # Output directory\n",
    "\n",
    "print(\"‚öôÔ∏è Processing Configuration:\")\n",
    "print(f\"   Mode: {processing_mode.mode}\")\n",
    "print(f\"   Combine barcodes: {processing_mode.combine_barcodes}\")\n",
    "print(f\"   Normalization: {processing_mode.normalization_method}\")\n",
    "print(f\"   Correlation threshold: {processing_mode.correlation_threshold}\")\n",
    "print(f\"   P-value threshold: {processing_mode.p_value_threshold}\")\n",
    "print(f\"   Data directory: {data_dir}\")\n",
    "print(f\"   Barcode directories: {barcode_dirs}\")\n",
    "print(f\"   Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Pipeline Execution\n",
    "\n",
    "Run the complete enhanced pipeline with FASTQ processing, statistical validation, and PDF generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:28:38,350 - enhanced_notebook_interface - INFO - Starting enhanced FASTQ processing for patient: Montana\n",
      "2025-08-04 01:28:38,351 - enhanced_notebook_interface - INFO - Processing mode: single_horse\n",
      "2025-08-04 01:28:38,352 - enhanced_notebook_interface - INFO - Barcode directories: ['barcode04', 'barcode05', 'barcode06']\n",
      "2025-08-04 01:28:38,352 - enhanced_notebook_interface - INFO - Step 1: Processing FASTQ files with taxonomic classification...\n",
      "2025-08-04 01:28:38,353 - real_fastq_processor - INFO - Initialized minimal classifier with 12 reference k-mers\n",
      "2025-08-04 01:28:38,353 - real_fastq_processor - INFO - Processing 3 barcode directories\n",
      "2025-08-04 01:28:38,353 - real_fastq_processor - INFO - Processing barcode04...\n",
      "2025-08-04 01:28:38,354 - real_fastq_processor - INFO - Found 171 FASTQ files in ../data/barcode04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced FASTQ-to-PDF Pipeline\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:28:39,024 - real_fastq_processor - INFO -   barcode04: 1110 reads, 101 classified, 12 species\n",
      "2025-08-04 01:28:39,024 - real_fastq_processor - INFO - Processing barcode05...\n",
      "2025-08-04 01:28:39,025 - real_fastq_processor - INFO - Found 158 FASTQ files in ../data/barcode05\n",
      "2025-08-04 01:28:39,907 - real_fastq_processor - INFO -   barcode05: 1009 reads, 126 classified, 11 species\n",
      "2025-08-04 01:28:39,908 - real_fastq_processor - INFO - Processing barcode06...\n",
      "2025-08-04 01:28:39,908 - real_fastq_processor - INFO - Found 168 FASTQ files in ../data/barcode06\n",
      "2025-08-04 01:28:40,859 - real_fastq_processor - INFO -   barcode06: 1209 reads, 166 classified, 12 species\n",
      "2025-08-04 01:28:40,861 - real_fastq_processor - INFO - Generated abundance table: 12 species, 3 barcodes\n",
      "2025-08-04 01:28:40,863 - real_fastq_processor - INFO - Successfully generated abundance CSV: enhanced_results/processed_abundance.csv\n",
      "2025-08-04 01:28:40,864 - real_fastq_processor - INFO -   Species: 12\n",
      "2025-08-04 01:28:40,864 - real_fastq_processor - INFO -   Barcodes: 3\n",
      "2025-08-04 01:28:40,864 - real_fastq_processor - INFO -   Total reads: 393\n",
      "2025-08-04 01:28:40,866 - enhanced_notebook_interface - INFO - Original CSV generated: 12 species, 3 barcodes\n",
      "2025-08-04 01:28:40,866 - enhanced_notebook_interface - INFO - Step 2: Aggregating barcodes for single patient...\n",
      "2025-08-04 01:28:40,867 - barcode_aggregator - INFO - Starting barcode aggregation for Montana\n",
      "2025-08-04 01:28:40,867 - barcode_aggregator - INFO - Combining barcodes: ['barcode04', 'barcode05', 'barcode06']\n",
      "2025-08-04 01:28:40,871 - barcode_aggregator - INFO - Valid correlation barcode04-barcode05: r=0.822, p=0.001\n",
      "2025-08-04 01:28:40,872 - barcode_aggregator - WARNING - Low correlation barcode04-barcode06: r=0.696, p=0.012\n",
      "2025-08-04 01:28:40,873 - barcode_aggregator - WARNING - Low correlation barcode05-barcode06: r=0.608, p=0.036\n",
      "2025-08-04 01:28:40,875 - barcode_aggregator - INFO - Applied relative_abundance normalization to 3 barcodes\n",
      "2025-08-04 01:28:40,878 - barcode_aggregator - INFO - Combined 3 barcodes into 12 species\n",
      "2025-08-04 01:28:40,880 - barcode_aggregator - INFO - Aggregation successful: 12 species, 299 total reads\n",
      "2025-08-04 01:28:40,881 - enhanced_notebook_interface - INFO - Barcode aggregation successful!\n",
      "2025-08-04 01:28:40,882 - enhanced_notebook_interface - INFO -    Combined CSV: enhanced_results/combined_abundance_montana.csv\n",
      "2025-08-04 01:28:40,882 - enhanced_notebook_interface - INFO -    Species: 12\n",
      "2025-08-04 01:28:40,882 - enhanced_notebook_interface - INFO -    Total reads: 299\n",
      "2025-08-04 01:28:40,883 - enhanced_notebook_interface - INFO -    Correlation barcode04_vs_barcode05: r=0.822, p=0.001\n",
      "2025-08-04 01:28:40,883 - enhanced_notebook_interface - INFO -    Correlation barcode04_vs_barcode06: r=0.696, p=0.012\n",
      "2025-08-04 01:28:40,883 - enhanced_notebook_interface - INFO -    Correlation barcode05_vs_barcode06: r=0.608, p=0.036\n",
      "2025-08-04 01:28:40,885 - enhanced_notebook_interface - INFO - Enhanced pipeline completed in 2.54 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced Pipeline Execution Successful!\n",
      "   Processing time: 2.54 seconds\n",
      "   Original species count: 12\n",
      "   Barcode count: 3\n",
      "   Combined species count: 12\n",
      "   CSV generated: enhanced_results/processed_abundance.csv\n",
      "   Combined CSV: enhanced_results/combined_abundance_montana.csv\n",
      "\n",
      "üìä Statistical Validation Results:\n",
      "   Combined total reads: 299\n",
      "   Normalization method: relative_abundance\n",
      "\n",
      "üìà Barcode Correlations:\n",
      "   ‚úÖ Valid: barcode04_vs_barcode05 (r=0.822, p=0.001)\n",
      "   ‚ö†Ô∏è Warning: barcode04_vs_barcode06 (r=0.696, p=0.012)\n",
      "   ‚ö†Ô∏è Warning: barcode05_vs_barcode06 (r=0.608, p=0.036)\n",
      "\n",
      "ü¶† Species Analysis:\n",
      "   Total unique species: 12\n",
      "   Common to all barcodes: 11\n",
      "\n",
      "‚ö†Ô∏è Quality Warnings:\n",
      "   ‚Ä¢ Low correlation barcode04-barcode06: r=0.696, p=0.012\n",
      "   ‚Ä¢ Low correlation barcode05-barcode06: r=0.608, p=0.036\n",
      "   ‚Ä¢ Some barcodes show poor correlation - may not represent same sample\n"
     ]
    }
   ],
   "source": [
    "# Execute enhanced FASTQ-to-PDF pipeline\n",
    "print(\"üöÄ Starting Enhanced FASTQ-to-PDF Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run enhanced pipeline\n",
    "result = run_enhanced_pipeline(\n",
    "    data_dir=data_dir,\n",
    "    barcode_dirs=barcode_dirs,\n",
    "    patient=patient,\n",
    "    output_dir=output_dir,\n",
    "    processing_mode=processing_mode\n",
    ")\n",
    "\n",
    "if result.success:\n",
    "    print(\"\\n‚úÖ Enhanced Pipeline Execution Successful!\")\n",
    "    print(f\"   Processing time: {result.total_processing_time:.2f} seconds\")\n",
    "    print(f\"   Original species count: {result.species_count}\")\n",
    "    print(f\"   Barcode count: {result.barcode_count}\")\n",
    "    print(f\"   Combined species count: {result.combined_species_count}\")\n",
    "    print(f\"   CSV generated: {result.csv_path}\")\n",
    "    \n",
    "    if result.combined_csv_path:\n",
    "        print(f\"   Combined CSV: {result.combined_csv_path}\")\n",
    "    \n",
    "    # Display aggregation results if available\n",
    "    if result.aggregation_result and result.aggregation_result.success:\n",
    "        agg = result.aggregation_result\n",
    "        print(f\"\\nüìä Statistical Validation Results:\")\n",
    "        print(f\"   Combined total reads: {agg.combined_total_reads}\")\n",
    "        print(f\"   Normalization method: {agg.normalization_stats.get('method', 'unknown')}\")\n",
    "        \n",
    "        # Show correlation results\n",
    "        print(f\"\\nüìà Barcode Correlations:\")\n",
    "        for pair, corr in agg.validation_result.correlations.items():\n",
    "            p_val = agg.validation_result.p_values[pair]\n",
    "            valid = \"‚úÖ Valid\" if corr >= processing_mode.correlation_threshold and p_val <= processing_mode.p_value_threshold else \"‚ö†Ô∏è Warning\"\n",
    "            print(f\"   {valid}: {pair} (r={corr:.3f}, p={p_val:.3f})\")\n",
    "        \n",
    "        # Show species overlap\n",
    "        overlap = agg.species_overlap\n",
    "        print(f\"\\nü¶† Species Analysis:\")\n",
    "        print(f\"   Total unique species: {overlap['total_unique_species']}\")\n",
    "        print(f\"   Common to all barcodes: {overlap['common_to_all_barcodes']}\")\n",
    "        \n",
    "        # Show quality warnings\n",
    "        if agg.validation_result.warnings:\n",
    "            print(f\"\\n‚ö†Ô∏è Quality Warnings:\")\n",
    "            for warning in agg.validation_result.warnings:\n",
    "                print(f\"   ‚Ä¢ {warning}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå Pipeline execution failed: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Report Generation\n",
    "\n",
    "Generate the final professional PDF report from the combined microbiome data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:28:53,273 - enhanced_notebook_interface - INFO - Generating report from combined CSV: combined_abundance_montana.csv\n",
      "2025-08-04 01:28:53,275 - notebook_pdf_generator - INFO - NotebookPDFGenerator initialized for language: en\n",
      "2025-08-04 01:28:53,276 - notebook_pdf_generator - INFO - Starting report generation for Montana\n",
      "2025-08-04 01:28:53,279 - notebook_pdf_generator - INFO - Processed 12 species\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Generating Professional PDF Report\n",
      "========================================\n",
      "üìÑ Report path: enhanced_results/Montana_enhanced_microbiome_report.pdf\n",
      "üìä Data source: enhanced_results/combined_abundance_montana.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:28:53,954 - notebook_pdf_generator - INFO - Generated 2 charts successfully\n",
      "2025-08-04 01:28:53,955 - notebook_pdf_generator - INFO - Generated 2 charts\n",
      "2025-08-04 01:28:53,958 - notebook_llm_engine - INFO - Environment variables loaded from /home/trentleslie/Insync/projects/equine-microbiome-reporter/.env (result: True)\n",
      "2025-08-04 01:29:12,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-04 01:29:12,261 - notebook_pdf_generator - INFO - Added 5 LLM-powered recommendations\n",
      "2025-08-04 01:29:12,728 - weasyprint - WARNING - Ignored `box-shadow: 0 2px 4px rgba(0,0,0,0.05)` at 113:13, unknown property.\n",
      "2025-08-04 01:29:12,743 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/dna_stock_photo.jpg\">\n",
      "2025-08-04 01:29:12,743 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/hippovet_logo.png\">\n",
      "2025-08-04 01:29:12,744 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/hippovet_logo.png\">\n",
      "2025-08-04 01:29:12,744 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"/home/trentleslie/Insync/projects/equine-microbiome-reporter/notebooks/temp_charts/species_distribution.png\">\n",
      "2025-08-04 01:29:12,744 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"/home/trentleslie/Insync/projects/equine-microbiome-reporter/notebooks/temp_charts/phylum_distribution.png\">\n",
      "2025-08-04 01:29:12,745 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/hippovet_logo.png\">\n",
      "2025-08-04 01:29:12,746 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/hippovet_logo.png\">\n",
      "2025-08-04 01:29:12,749 - weasyprint - ERROR - Relative URI reference without a base URI: <img src=\"assets/hippovet_logo.png\">\n",
      "2025-08-04 01:29:13,072 - fontTools.ttLib.ttFont - DEBUG - Reading 'maxp' table from disk\n",
      "2025-08-04 01:29:13,072 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'maxp' table\n",
      "2025-08-04 01:29:13,072 - fontTools.subset.timer - DEBUG - Took 0.001s to load 'maxp'\n",
      "2025-08-04 01:29:13,073 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'maxp'\n",
      "2025-08-04 01:29:13,073 - fontTools.subset - INFO - maxp pruned\n",
      "2025-08-04 01:29:13,074 - fontTools.ttLib.ttFont - DEBUG - Reading 'cmap' table from disk\n",
      "2025-08-04 01:29:13,075 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'cmap' table\n",
      "2025-08-04 01:29:13,076 - fontTools.ttLib.ttFont - DEBUG - Reading 'post' table from disk\n",
      "2025-08-04 01:29:13,076 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'post' table\n",
      "2025-08-04 01:29:13,079 - fontTools.subset.timer - DEBUG - Took 0.004s to load 'cmap'\n",
      "2025-08-04 01:29:13,079 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'cmap'\n",
      "2025-08-04 01:29:13,079 - fontTools.subset - INFO - cmap pruned\n",
      "2025-08-04 01:29:13,079 - fontTools.subset - INFO - fpgm dropped\n",
      "2025-08-04 01:29:13,080 - fontTools.subset - INFO - prep dropped\n",
      "2025-08-04 01:29:13,080 - fontTools.subset - INFO - cvt  dropped\n",
      "2025-08-04 01:29:13,080 - fontTools.subset - INFO - kern dropped\n",
      "2025-08-04 01:29:13,081 - fontTools.subset.timer - DEBUG - Took 0.000s to load 'post'\n",
      "2025-08-04 01:29:13,081 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'post'\n",
      "2025-08-04 01:29:13,081 - fontTools.subset - INFO - post pruned\n",
      "2025-08-04 01:29:13,082 - fontTools.subset - INFO - GPOS dropped\n",
      "2025-08-04 01:29:13,082 - fontTools.subset - INFO - GSUB dropped\n",
      "2025-08-04 01:29:13,083 - fontTools.ttLib.ttFont - DEBUG - Reading 'glyf' table from disk\n",
      "2025-08-04 01:29:13,083 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'glyf' table\n",
      "2025-08-04 01:29:13,083 - fontTools.ttLib.ttFont - DEBUG - Reading 'loca' table from disk\n",
      "2025-08-04 01:29:13,084 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'loca' table\n",
      "2025-08-04 01:29:13,084 - fontTools.ttLib.ttFont - DEBUG - Reading 'head' table from disk\n",
      "2025-08-04 01:29:13,084 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'head' table\n",
      "2025-08-04 01:29:13,086 - fontTools.subset.timer - DEBUG - Took 0.003s to load 'glyf'\n",
      "2025-08-04 01:29:13,086 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'glyf'\n",
      "2025-08-04 01:29:13,086 - fontTools.subset - INFO - glyf pruned\n",
      "2025-08-04 01:29:13,087 - fontTools.subset.timer - DEBUG - Took 0.000s to close glyph list over 'cmap'\n",
      "2025-08-04 01:29:13,087 - fontTools.subset - INFO - Added gid0 to subset\n",
      "2025-08-04 01:29:13,087 - fontTools.subset - INFO - Closing glyph list over 'glyf': 68 glyphs before\n",
      "2025-08-04 01:29:13,088 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'C', 'D', 'F', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'a', 'b', 'c', 'colon', 'comma', 'd', 'e', 'eight', 'f', 'five', 'four', 'g', 'greater', 'h', 'hyphen', 'i', 'j', 'k', 'l', 'm', 'n', 'nacute', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'plus', 'q', 'r', 's', 'semicolon', 'seven', 'six', 'slash', 'space', 't', 'three', 'two', 'u', 'underscore', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-08-04 01:29:13,088 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 8, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 36, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 262]\n",
      "2025-08-04 01:29:13,088 - fontTools.subset - INFO - Closed glyph list over 'glyf': 69 glyphs after\n",
      "2025-08-04 01:29:13,089 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'C', 'D', 'F', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'a', 'acute', 'b', 'c', 'colon', 'comma', 'd', 'e', 'eight', 'f', 'five', 'four', 'g', 'greater', 'h', 'hyphen', 'i', 'j', 'k', 'l', 'm', 'n', 'nacute', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'plus', 'q', 'r', 's', 'semicolon', 'seven', 'six', 'slash', 'space', 't', 'three', 'two', 'u', 'underscore', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-08-04 01:29:13,089 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 8, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 36, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 118, 262]\n",
      "2025-08-04 01:29:13,089 - fontTools.subset.timer - DEBUG - Took 0.002s to close glyph list over 'glyf'\n",
      "2025-08-04 01:29:13,090 - fontTools.subset - INFO - Retaining 69 glyphs\n",
      "2025-08-04 01:29:13,090 - fontTools.subset - INFO - head subsetting not needed\n",
      "2025-08-04 01:29:13,090 - fontTools.subset - INFO - hhea subsetting not needed\n",
      "2025-08-04 01:29:13,091 - fontTools.subset - INFO - maxp subsetting not needed\n",
      "2025-08-04 01:29:13,091 - fontTools.subset - INFO - OS/2 subsetting not needed\n",
      "2025-08-04 01:29:13,092 - fontTools.ttLib.ttFont - DEBUG - Reading 'hmtx' table from disk\n",
      "2025-08-04 01:29:13,092 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hmtx' table\n",
      "2025-08-04 01:29:13,092 - fontTools.ttLib.ttFont - DEBUG - Reading 'hhea' table from disk\n",
      "2025-08-04 01:29:13,092 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hhea' table\n",
      "2025-08-04 01:29:13,094 - fontTools.subset.timer - DEBUG - Took 0.002s to subset 'hmtx'\n",
      "2025-08-04 01:29:13,094 - fontTools.subset - INFO - hmtx subsetted\n",
      "2025-08-04 01:29:13,094 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'cmap'\n",
      "2025-08-04 01:29:13,095 - fontTools.subset - INFO - cmap subsetted\n",
      "2025-08-04 01:29:13,095 - fontTools.subset - INFO - loca subsetting not needed\n",
      "2025-08-04 01:29:13,095 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'post'\n",
      "2025-08-04 01:29:13,095 - fontTools.subset - INFO - post subsetted\n",
      "2025-08-04 01:29:13,095 - fontTools.subset - INFO - gasp subsetting not needed\n",
      "2025-08-04 01:29:13,095 - fontTools.subset - INFO - FFTM NOT subset; don't know how to subset\n",
      "2025-08-04 01:29:13,095 - fontTools.ttLib.ttFont - DEBUG - Reading 'GDEF' table from disk\n",
      "2025-08-04 01:29:13,096 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'GDEF' table\n",
      "2025-08-04 01:29:13,096 - fontTools.subset.timer - DEBUG - Took 0.001s to subset 'GDEF'\n",
      "2025-08-04 01:29:13,097 - fontTools.subset - INFO - GDEF subsetted\n",
      "2025-08-04 01:29:13,097 - fontTools.subset - INFO - name subsetting not needed\n",
      "2025-08-04 01:29:13,098 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'glyf'\n",
      "2025-08-04 01:29:13,098 - fontTools.subset - INFO - glyf subsetted\n",
      "2025-08-04 01:29:13,098 - fontTools.subset.timer - DEBUG - Took 0.000s to subset GlyphOrder\n",
      "2025-08-04 01:29:13,099 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'head'\n",
      "2025-08-04 01:29:13,099 - fontTools.subset - INFO - head pruned\n",
      "2025-08-04 01:29:13,099 - fontTools.ttLib.ttFont - DEBUG - Reading 'OS/2' table from disk\n",
      "2025-08-04 01:29:13,099 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'OS/2' table\n",
      "2025-08-04 01:29:13,099 - fontTools.subset - INFO - OS/2 Unicode ranges pruned: [0, 2]\n",
      "2025-08-04 01:29:13,100 - fontTools.subset - INFO - OS/2 CodePage ranges pruned: [0]\n",
      "2025-08-04 01:29:13,100 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'glyf'\n",
      "2025-08-04 01:29:13,101 - fontTools.subset - INFO - glyf pruned\n",
      "2025-08-04 01:29:13,101 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'GDEF'\n",
      "2025-08-04 01:29:13,101 - fontTools.subset - INFO - GDEF pruned\n",
      "2025-08-04 01:29:13,101 - fontTools.ttLib.ttFont - DEBUG - Reading 'name' table from disk\n",
      "2025-08-04 01:29:13,102 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'name' table\n",
      "2025-08-04 01:29:13,102 - fontTools.ttLib.ttFont - DEBUG - Reading 'gasp' table from disk\n",
      "2025-08-04 01:29:13,103 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'gasp' table\n",
      "2025-08-04 01:29:13,103 - fontTools.ttLib.ttFont - DEBUG - Reading 'FFTM' table from disk\n",
      "2025-08-04 01:29:13,103 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'FFTM' table\n",
      "2025-08-04 01:29:13,104 - fontTools.subset.timer - DEBUG - Took 0.002s to prune 'name'\n",
      "2025-08-04 01:29:13,104 - fontTools.subset - INFO - name pruned\n",
      "2025-08-04 01:29:13,110 - fontTools.ttLib.ttFont - DEBUG - Reading 'maxp' table from disk\n",
      "2025-08-04 01:29:13,110 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'maxp' table\n",
      "2025-08-04 01:29:13,110 - fontTools.subset.timer - DEBUG - Took 0.001s to load 'maxp'\n",
      "2025-08-04 01:29:13,111 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'maxp'\n",
      "2025-08-04 01:29:13,111 - fontTools.subset - INFO - maxp pruned\n",
      "2025-08-04 01:29:13,112 - fontTools.ttLib.ttFont - DEBUG - Reading 'cmap' table from disk\n",
      "2025-08-04 01:29:13,112 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'cmap' table\n",
      "2025-08-04 01:29:13,113 - fontTools.ttLib.ttFont - DEBUG - Reading 'post' table from disk\n",
      "2025-08-04 01:29:13,113 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'post' table\n",
      "2025-08-04 01:29:13,115 - fontTools.subset.timer - DEBUG - Took 0.004s to load 'cmap'\n",
      "2025-08-04 01:29:13,116 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'cmap'\n",
      "2025-08-04 01:29:13,116 - fontTools.subset - INFO - cmap pruned\n",
      "2025-08-04 01:29:13,117 - fontTools.subset - INFO - fpgm dropped\n",
      "2025-08-04 01:29:13,117 - fontTools.subset - INFO - prep dropped\n",
      "2025-08-04 01:29:13,117 - fontTools.subset - INFO - cvt  dropped\n",
      "2025-08-04 01:29:13,117 - fontTools.subset - INFO - kern dropped\n",
      "2025-08-04 01:29:13,118 - fontTools.subset.timer - DEBUG - Took 0.000s to load 'post'\n",
      "2025-08-04 01:29:13,118 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'post'\n",
      "2025-08-04 01:29:13,119 - fontTools.subset - INFO - post pruned\n",
      "2025-08-04 01:29:13,119 - fontTools.subset - INFO - GPOS dropped\n",
      "2025-08-04 01:29:13,120 - fontTools.subset - INFO - GSUB dropped\n",
      "2025-08-04 01:29:13,120 - fontTools.ttLib.ttFont - DEBUG - Reading 'glyf' table from disk\n",
      "2025-08-04 01:29:13,121 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'glyf' table\n",
      "2025-08-04 01:29:13,121 - fontTools.ttLib.ttFont - DEBUG - Reading 'loca' table from disk\n",
      "2025-08-04 01:29:13,121 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'loca' table\n",
      "2025-08-04 01:29:13,121 - fontTools.ttLib.ttFont - DEBUG - Reading 'head' table from disk\n",
      "2025-08-04 01:29:13,122 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'head' table\n",
      "2025-08-04 01:29:13,124 - fontTools.subset.timer - DEBUG - Took 0.004s to load 'glyf'\n",
      "2025-08-04 01:29:13,124 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'glyf'\n",
      "2025-08-04 01:29:13,124 - fontTools.subset - INFO - glyf pruned\n",
      "2025-08-04 01:29:13,125 - fontTools.subset.timer - DEBUG - Took 0.000s to close glyph list over 'cmap'\n",
      "2025-08-04 01:29:13,125 - fontTools.subset - INFO - Added gid0 to subset\n",
      "2025-08-04 01:29:13,125 - fontTools.subset - INFO - Closing glyph list over 'glyf': 50 glyphs before\n",
      "2025-08-04 01:29:13,126 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'a', 'b', 'c', 'colon', 'd', 'e', 'g', 'hyphen', 'i', 'm', 'n', 'o', 'p', 'parenleft', 'parenright', 'period', 'r', 's', 'seven', 'six', 'space', 't', 'three', 'two', 'v', 'x', 'y']\n",
      "2025-08-04 01:29:13,126 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 11, 12, 16, 17, 21, 22, 25, 26, 29, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 68, 69, 70, 71, 72, 74, 76, 80, 81, 82, 83, 85, 86, 87, 89, 91, 92]\n",
      "2025-08-04 01:29:13,126 - fontTools.subset - INFO - Closed glyph list over 'glyf': 50 glyphs after\n",
      "2025-08-04 01:29:13,127 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'a', 'b', 'c', 'colon', 'd', 'e', 'g', 'hyphen', 'i', 'm', 'n', 'o', 'p', 'parenleft', 'parenright', 'period', 'r', 's', 'seven', 'six', 'space', 't', 'three', 'two', 'v', 'x', 'y']\n",
      "2025-08-04 01:29:13,127 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 11, 12, 16, 17, 21, 22, 25, 26, 29, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 68, 69, 70, 71, 72, 74, 76, 80, 81, 82, 83, 85, 86, 87, 89, 91, 92]\n",
      "2025-08-04 01:29:13,127 - fontTools.subset.timer - DEBUG - Took 0.002s to close glyph list over 'glyf'\n",
      "2025-08-04 01:29:13,128 - fontTools.subset - INFO - Retaining 50 glyphs\n",
      "2025-08-04 01:29:13,128 - fontTools.subset - INFO - head subsetting not needed\n",
      "2025-08-04 01:29:13,129 - fontTools.subset - INFO - hhea subsetting not needed\n",
      "2025-08-04 01:29:13,129 - fontTools.subset - INFO - maxp subsetting not needed\n",
      "2025-08-04 01:29:13,129 - fontTools.subset - INFO - OS/2 subsetting not needed\n",
      "2025-08-04 01:29:13,130 - fontTools.ttLib.ttFont - DEBUG - Reading 'hmtx' table from disk\n",
      "2025-08-04 01:29:13,130 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hmtx' table\n",
      "2025-08-04 01:29:13,130 - fontTools.ttLib.ttFont - DEBUG - Reading 'hhea' table from disk\n",
      "2025-08-04 01:29:13,131 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hhea' table\n",
      "2025-08-04 01:29:13,132 - fontTools.subset.timer - DEBUG - Took 0.002s to subset 'hmtx'\n",
      "2025-08-04 01:29:13,132 - fontTools.subset - INFO - hmtx subsetted\n",
      "2025-08-04 01:29:13,133 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'cmap'\n",
      "2025-08-04 01:29:13,133 - fontTools.subset - INFO - cmap subsetted\n",
      "2025-08-04 01:29:13,134 - fontTools.subset - INFO - loca subsetting not needed\n",
      "2025-08-04 01:29:13,134 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'post'\n",
      "2025-08-04 01:29:13,135 - fontTools.subset - INFO - post subsetted\n",
      "2025-08-04 01:29:13,136 - fontTools.subset - INFO - gasp subsetting not needed\n",
      "2025-08-04 01:29:13,137 - fontTools.subset - INFO - FFTM NOT subset; don't know how to subset\n",
      "2025-08-04 01:29:13,138 - fontTools.ttLib.ttFont - DEBUG - Reading 'GDEF' table from disk\n",
      "2025-08-04 01:29:13,138 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'GDEF' table\n",
      "2025-08-04 01:29:13,140 - fontTools.subset.timer - DEBUG - Took 0.002s to subset 'GDEF'\n",
      "2025-08-04 01:29:13,140 - fontTools.subset - INFO - GDEF subsetted\n",
      "2025-08-04 01:29:13,140 - fontTools.subset - INFO - name subsetting not needed\n",
      "2025-08-04 01:29:13,141 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'glyf'\n",
      "2025-08-04 01:29:13,142 - fontTools.subset - INFO - glyf subsetted\n",
      "2025-08-04 01:29:13,143 - fontTools.subset.timer - DEBUG - Took 0.000s to subset GlyphOrder\n",
      "2025-08-04 01:29:13,143 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'head'\n",
      "2025-08-04 01:29:13,144 - fontTools.subset - INFO - head pruned\n",
      "2025-08-04 01:29:13,144 - fontTools.ttLib.ttFont - DEBUG - Reading 'OS/2' table from disk\n",
      "2025-08-04 01:29:13,145 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'OS/2' table\n",
      "2025-08-04 01:29:13,145 - fontTools.subset - INFO - OS/2 Unicode ranges pruned: [0]\n",
      "2025-08-04 01:29:13,146 - fontTools.subset - INFO - OS/2 CodePage ranges pruned: [0]\n",
      "2025-08-04 01:29:13,147 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'glyf'\n",
      "2025-08-04 01:29:13,148 - fontTools.subset - INFO - glyf pruned\n",
      "2025-08-04 01:29:13,149 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'GDEF'\n",
      "2025-08-04 01:29:13,149 - fontTools.subset - INFO - GDEF pruned\n",
      "2025-08-04 01:29:13,149 - fontTools.ttLib.ttFont - DEBUG - Reading 'name' table from disk\n",
      "2025-08-04 01:29:13,150 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'name' table\n",
      "2025-08-04 01:29:13,150 - fontTools.ttLib.ttFont - DEBUG - Reading 'gasp' table from disk\n",
      "2025-08-04 01:29:13,151 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'gasp' table\n",
      "2025-08-04 01:29:13,151 - fontTools.ttLib.ttFont - DEBUG - Reading 'FFTM' table from disk\n",
      "2025-08-04 01:29:13,151 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'FFTM' table\n",
      "2025-08-04 01:29:13,152 - fontTools.subset.timer - DEBUG - Took 0.002s to prune 'name'\n",
      "2025-08-04 01:29:13,152 - fontTools.subset - INFO - name pruned\n",
      "2025-08-04 01:29:13,159 - notebook_pdf_generator - INFO - PDF report generated: enhanced_results/Montana_enhanced_microbiome_report.pdf\n",
      "2025-08-04 01:29:13,160 - notebook_pdf_generator - INFO - Temporary chart files cleaned up\n",
      "2025-08-04 01:29:13,160 - enhanced_notebook_interface - INFO - PDF report generated successfully: enhanced_results/Montana_enhanced_microbiome_report.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ PDF Report Generated Successfully!\n",
      "   File: enhanced_results/Montana_enhanced_microbiome_report.pdf\n",
      "   Size: 22.6 KB\n",
      "   Patient: Montana\n",
      "   Sample: 004-006_combined\n",
      "   Combined from: barcode04-barcode06\n"
     ]
    }
   ],
   "source": [
    "# Generate PDF report from combined data\n",
    "if result.success and result.combined_csv_path:\n",
    "    print(\"üìÑ Generating Professional PDF Report\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define PDF output path\n",
    "    pdf_filename = f\"{patient.name}_enhanced_microbiome_report.pdf\"\n",
    "    pdf_path = Path(output_dir) / pdf_filename\n",
    "    \n",
    "    print(f\"üìÑ Report path: {pdf_path}\")\n",
    "    print(f\"üìä Data source: {result.combined_csv_path}\")\n",
    "    \n",
    "    # Generate PDF report using combined data\n",
    "    pdf_success = generate_simple_pdf_report(\n",
    "        csv_path=result.combined_csv_path,\n",
    "        patient_info=patient,\n",
    "        output_path=str(pdf_path),\n",
    "        barcode_column=\"total\"  # Use combined total column\n",
    "    )\n",
    "    \n",
    "    if pdf_success and pdf_path.exists():\n",
    "        file_size = pdf_path.stat().st_size / 1024\n",
    "        print(f\"\\n‚úÖ PDF Report Generated Successfully!\")\n",
    "        print(f\"   File: {pdf_path}\")\n",
    "        print(f\"   Size: {file_size:.1f} KB\")\n",
    "        print(f\"   Patient: {patient.name}\")\n",
    "        print(f\"   Sample: {patient.sample_number}\")\n",
    "        print(f\"   Combined from: {patient.barcode_range}\")\n",
    "    else:\n",
    "        print(f\"‚ùå PDF generation failed\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping PDF generation - no combined CSV available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "Analyze the combined microbiome data and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Combined Microbiome Data Analysis\n",
      "========================================\n",
      "ü¶† Species Summary:\n",
      "   Total species: 12\n",
      "   Total combined abundance: 300.00%\n",
      "   Average abundance per species: 25.00%\n",
      "\n",
      "üèÜ Top 10 Most Abundant Species:\n",
      "    1. Bacteroides fragilis            49.95%  (Bacteroidota)\n",
      "    2. Lactobacillus acidophilus       45.97%  (Bacillota)\n",
      "    3. Fibrobacter succinogenes        43.55%  (Fibrobacterota)\n",
      "    4. Prevotella copri                32.46%  (Bacteroidota)\n",
      "    5. Salmonella enterica             22.32%  (Pseudomonadota)\n",
      "    6. Bacillus subtilis               20.27%  (Bacillota)\n",
      "    7. Escherichia coli                18.89%  (Pseudomonadota)\n",
      "    8. Enterococcus faecalis           17.94%  (Bacillota)\n",
      "    9. Streptomyces albidoflavus       14.96%  (Actinomycetota)\n",
      "   10. Clostridium perfringens         13.55%  (Bacillota)\n",
      "\n",
      "üß¨ Phylum Distribution:\n",
      "   Bacillota             97.72%\n",
      "   Bacteroidota          82.41%\n",
      "   Fibrobacterota        43.55%\n",
      "   Pseudomonadota        41.20%\n",
      "   Actinomycetota        35.11%\n",
      "\n",
      "üìã Data Structure:\n",
      "   Columns: ['species', 'phylum', 'genus', 'total_combined', 'total']\n",
      "   Shape: (12, 5)\n",
      "\n",
      "üìÑ Sample Data (Top 5 Species):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>phylum</th>\n",
       "      <th>genus</th>\n",
       "      <th>total_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacteroides fragilis</td>\n",
       "      <td>Bacteroidota</td>\n",
       "      <td>Bacteroides</td>\n",
       "      <td>49.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lactobacillus acidophilus</td>\n",
       "      <td>Bacillota</td>\n",
       "      <td>Lactobacillus</td>\n",
       "      <td>45.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fibrobacter succinogenes</td>\n",
       "      <td>Fibrobacterota</td>\n",
       "      <td>Fibrobacter</td>\n",
       "      <td>43.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prevotella copri</td>\n",
       "      <td>Bacteroidota</td>\n",
       "      <td>Prevotella</td>\n",
       "      <td>32.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salmonella enterica</td>\n",
       "      <td>Pseudomonadota</td>\n",
       "      <td>Salmonella</td>\n",
       "      <td>22.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     species          phylum          genus  total_combined\n",
       "0       Bacteroides fragilis    Bacteroidota    Bacteroides           49.95\n",
       "1  Lactobacillus acidophilus       Bacillota  Lactobacillus           45.97\n",
       "2   Fibrobacter succinogenes  Fibrobacterota    Fibrobacter           43.55\n",
       "3           Prevotella copri    Bacteroidota     Prevotella           32.46\n",
       "4        Salmonella enterica  Pseudomonadota     Salmonella           22.32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and analyze combined microbiome data\n",
    "if result.success and result.combined_csv_path:\n",
    "    print(\"üìä Combined Microbiome Data Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load combined data\n",
    "    combined_df = pd.read_csv(result.combined_csv_path)\n",
    "    \n",
    "    print(f\"ü¶† Species Summary:\")\n",
    "    print(f\"   Total species: {len(combined_df)}\")\n",
    "    print(f\"   Total combined abundance: {combined_df['total_combined'].sum():.2f}%\")\n",
    "    print(f\"   Average abundance per species: {combined_df['total_combined'].mean():.2f}%\")\n",
    "    \n",
    "    # Top 10 species\n",
    "    print(f\"\\nüèÜ Top 10 Most Abundant Species:\")\n",
    "    top_species = combined_df.head(10)\n",
    "    for i, (_, row) in enumerate(top_species.iterrows(), 1):\n",
    "        print(f\"   {i:2d}. {row['species']:<30} {row['total_combined']:>6.2f}%  ({row['phylum']})\")\n",
    "    \n",
    "    # Phylum distribution\n",
    "    print(f\"\\nüß¨ Phylum Distribution:\")\n",
    "    phylum_dist = combined_df.groupby('phylum')['total_combined'].sum().sort_values(ascending=False)\n",
    "    for phylum, abundance in phylum_dist.items():\n",
    "        print(f\"   {phylum:<20} {abundance:>6.2f}%\")\n",
    "    \n",
    "    # Display dataframe structure\n",
    "    print(f\"\\nüìã Data Structure:\")\n",
    "    print(f\"   Columns: {list(combined_df.columns)}\")\n",
    "    print(f\"   Shape: {combined_df.shape}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nüìÑ Sample Data (Top 5 Species):\")\n",
    "    display(combined_df.head()[['species', 'phylum', 'genus', 'total_combined']].round(2))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No combined data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control Summary\n",
    "\n",
    "Display comprehensive quality control metrics and validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Quality Control Summary\n",
      "==============================\n",
      "üìä Original Data Quality:\n",
      "   barcode04:\n",
      "     Total reads: 101\n",
      "     Species count: 12\n",
      "     Max abundance: 19.0\n",
      "     Top species: Lactobacillus acidophilus\n",
      "   barcode05:\n",
      "     Total reads: 126\n",
      "     Species count: 11\n",
      "     Max abundance: 31.0\n",
      "     Top species: Bacteroides fragilis\n",
      "   barcode06:\n",
      "     Total reads: 166\n",
      "     Species count: 12\n",
      "     Max abundance: 24.0\n",
      "     Top species: Bacteroides fragilis\n",
      "\n",
      "üî¨ Aggregation Quality:\n",
      "   Status: ‚úÖ Success\n",
      "   Normalization: relative_abundance\n",
      "   Combined species: 12\n",
      "   Combined reads: 299\n",
      "\n",
      "üìà Correlation Summary:\n",
      "   ‚úÖ barcode04_vs_barcode05: r=0.822, p=0.001\n",
      "   ‚ö†Ô∏è barcode04_vs_barcode06: r=0.696, p=0.012\n",
      "   ‚ö†Ô∏è barcode05_vs_barcode06: r=0.608, p=0.036\n",
      "\n",
      "üí° Recommendations:\n",
      "   ‚Ä¢ Review barcode correlation warnings\n",
      "\n",
      "‚è∞ Analysis completed: 2025-08-04 01:28:40\n"
     ]
    }
   ],
   "source": [
    "# Display quality control summary\n",
    "if result.success and result.quality_report:\n",
    "    print(\"üîç Quality Control Summary\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    qc = result.quality_report\n",
    "    \n",
    "    # Original data quality\n",
    "    print(f\"üìä Original Data Quality:\")\n",
    "    if 'original_data' in qc:\n",
    "        for barcode, metrics in qc['original_data'].items():\n",
    "            print(f\"   {barcode}:\")\n",
    "            print(f\"     Total reads: {metrics['total_reads']:,}\")\n",
    "            print(f\"     Species count: {metrics['species_count']}\")\n",
    "            print(f\"     Max abundance: {metrics['max_abundance']:.1f}\")\n",
    "            print(f\"     Top species: {metrics['top_species']}\")\n",
    "    \n",
    "    # Aggregation quality\n",
    "    print(f\"\\nüî¨ Aggregation Quality:\")\n",
    "    if 'aggregation' in qc and qc['aggregation']['success']:\n",
    "        agg_qc = qc['aggregation']\n",
    "        print(f\"   Status: ‚úÖ Success\")\n",
    "        print(f\"   Normalization: {agg_qc['normalization_method']}\")\n",
    "        print(f\"   Combined species: {agg_qc['combined_species_count']}\")\n",
    "        print(f\"   Combined reads: {agg_qc['combined_total_reads']}\")\n",
    "        \n",
    "        # Correlation summary\n",
    "        if 'correlations' in agg_qc:\n",
    "            print(f\"\\nüìà Correlation Summary:\")\n",
    "            for pair, corr in agg_qc['correlations'].items():\n",
    "                p_val = agg_qc['p_values'][pair]\n",
    "                status = \"‚úÖ\" if corr >= 0.7 and p_val <= 0.05 else \"‚ö†Ô∏è\"\n",
    "                print(f\"   {status} {pair}: r={corr:.3f}, p={p_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"   Status: ‚ùå Failed or not performed\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if 'recommendations' in qc and qc['recommendations']:\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        for rec in qc['recommendations']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    print(f\"\\n‚è∞ Analysis completed: {qc.get('timestamp', 'Unknown')}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No quality control data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "Final summary of the enhanced pipeline execution and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Enhanced Pipeline Summary\n",
      "========================================\n",
      "‚úÖ Pipeline Status: SUCCESS\n",
      "üìä Processing Summary:\n",
      "   Patient: Montana\n",
      "   Sample: 004-006_combined\n",
      "   Barcodes processed: 3\n",
      "   Original species: 12\n",
      "   Combined species: 12\n",
      "   Processing time: 2.54s\n",
      "\n",
      "üìÅ Output Files:\n",
      "   Original CSV: enhanced_results/processed_abundance.csv\n",
      "   Combined CSV: enhanced_results/combined_abundance_montana.csv\n",
      "   PDF Report: enhanced_results/Montana_enhanced_microbiome_report.pdf\n",
      "\n",
      "üî¨ Enhanced Features:\n",
      "   ‚úÖ Single horse analysis (not multiple reports)\n",
      "   ‚úÖ Statistical validation of technical replicates\n",
      "   ‚úÖ Data normalization (relative_abundance)\n",
      "   ‚úÖ Quality control metrics and warnings\n",
      "   ‚úÖ Comprehensive error handling and logging\n",
      "   ‚úÖ Professional PDF report generation\n",
      "\n",
      "üìà Validation Summary:\n",
      "   Valid correlations: 1/3\n",
      "   Correlation threshold: r ‚â• 0.7\n",
      "   Significance threshold: p ‚â§ 0.05\n",
      "\n",
      "‚è∞ Session completed: 2025-08-04 01:29:27\n",
      "\n",
      "============================================================\n",
      "üêé Enhanced FASTQ-to-PDF Pipeline - Complete\n",
      "Single Horse ‚Ä¢ Statistical Validation ‚Ä¢ Professional Reports\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final pipeline summary\n",
    "print(\"üéØ Enhanced Pipeline Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if result.success:\n",
    "    print(f\"‚úÖ Pipeline Status: SUCCESS\")\n",
    "    print(f\"üìä Processing Summary:\")\n",
    "    print(f\"   Patient: {patient.name}\")\n",
    "    print(f\"   Sample: {patient.sample_number}\")\n",
    "    print(f\"   Barcodes processed: {result.barcode_count}\")\n",
    "    print(f\"   Original species: {result.species_count}\")\n",
    "    print(f\"   Combined species: {result.combined_species_count}\")\n",
    "    print(f\"   Processing time: {result.total_processing_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Output Files:\")\n",
    "    if result.csv_path:\n",
    "        print(f\"   Original CSV: {result.csv_path}\")\n",
    "    if result.combined_csv_path:\n",
    "        print(f\"   Combined CSV: {result.combined_csv_path}\")\n",
    "    \n",
    "    pdf_path = Path(output_dir) / f\"{patient.name}_enhanced_microbiome_report.pdf\"\n",
    "    if pdf_path.exists():\n",
    "        print(f\"   PDF Report: {pdf_path}\")\n",
    "    \n",
    "    print(f\"\\nüî¨ Enhanced Features:\")\n",
    "    print(f\"   ‚úÖ Single horse analysis (not multiple reports)\")\n",
    "    print(f\"   ‚úÖ Statistical validation of technical replicates\")\n",
    "    print(f\"   ‚úÖ Data normalization ({processing_mode.normalization_method})\")\n",
    "    print(f\"   ‚úÖ Quality control metrics and warnings\")\n",
    "    print(f\"   ‚úÖ Comprehensive error handling and logging\")\n",
    "    print(f\"   ‚úÖ Professional PDF report generation\")\n",
    "    \n",
    "    # Validation summary\n",
    "    if result.aggregation_result and result.aggregation_result.success:\n",
    "        valid_pairs = len([c for c in result.aggregation_result.validation_result.correlations.values() \n",
    "                          if c >= processing_mode.correlation_threshold])\n",
    "        total_pairs = len(result.aggregation_result.validation_result.correlations)\n",
    "        print(f\"\\nüìà Validation Summary:\")\n",
    "        print(f\"   Valid correlations: {valid_pairs}/{total_pairs}\")\n",
    "        print(f\"   Correlation threshold: r ‚â• {processing_mode.correlation_threshold}\")\n",
    "        print(f\"   Significance threshold: p ‚â§ {processing_mode.p_value_threshold}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Pipeline Status: FAILED\")\n",
    "    print(f\"   Error: {result.error}\")\n",
    "    print(f\"   Processing time: {result.total_processing_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚è∞ Session completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üêé Enhanced FASTQ-to-PDF Pipeline - Complete\")\n",
    "print(\"Single Horse ‚Ä¢ Statistical Validation ‚Ä¢ Professional Reports\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equine-microbiome-reporter-GKwo-_Mf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
