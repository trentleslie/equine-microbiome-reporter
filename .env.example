# LLM API Configuration
# Copy this file to .env and fill in your API keys

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4  # Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo

# Anthropic Configuration (Claude)
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-opus-20240229  # Options: claude-3-opus, claude-3-sonnet, claude-3-haiku

# Google Gemini Configuration
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-1.5-pro  # Options: gemini-1.5-pro, gemini-1.5-flash

# LLM Provider Selection
LLM_PROVIDER=openai  # Options: openai, anthropic, gemini

# LLM Parameters
LLM_TEMPERATURE=0.3  # Lower values for more consistent output
LLM_MAX_TOKENS=1500
LLM_TIMEOUT=60  # Timeout in seconds

# Feature Flags
ENABLE_LLM_RECOMMENDATIONS=false  # Set to true to enable LLM features
LLM_CACHE_ENABLED=true  # Cache LLM responses to reduce API calls
LLM_CACHE_TTL=86400  # Cache time-to-live in seconds (24 hours)

# Logging
LLM_LOG_REQUESTS=true  # Log all LLM requests for debugging
LLM_LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR